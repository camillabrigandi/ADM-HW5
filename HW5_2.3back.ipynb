{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import heapq\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "import pickle\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"citation.pkl\",'rb')\n",
    "citation_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = file = open(\"collaboration.pkl\",'rb')\n",
    "collaboration_data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining class graph for next functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph:\n",
    "    def __init__(self, V: list, E: list, attributes: dict, directed: bool):\n",
    "        self.nodes = V\n",
    "        self.edges = E\n",
    "        self.edges_attributes = attributes\n",
    "        \n",
    "        # Flag to indicate if the graph is directed\n",
    "        self.isdirected = directed\n",
    "    \n",
    "    def set_edges_attribute(self, edges, values, name=None):\n",
    "        attribute_dict = dict.fromkeys(self.edges, None)\n",
    "        for (edge, value) in zip(edges, values):\n",
    "            attribute_dict[edge] = value\n",
    "            self.edges_attributes.update({name : attribute_dict})\n",
    "    \n",
    "    def extract_subgraph(self, subgraph_nodes):\n",
    "        # The subgraph will be directed iff the original graph is directed\n",
    "    \n",
    "        # retrieve edges with both end in the list of nodes of the subgraph\n",
    "        subgraph_edges = [edge for edge in self.edges if (edge[0] in subgraph_nodes and edge[1] in subgraph_nodes) ]\n",
    "\n",
    "        # Update attributes for the subraph\n",
    "        subgraph_attributes = dict()\n",
    "        for attribute in self.edges_attributes: \n",
    "            original_attribute = self.edges_attributes[attribute]\n",
    "            subgraph_attributes[attribute] = {edge: original_attribute[edge] for edge in subgraph_edges}\n",
    "\n",
    "        subgraph = graph(subgraph_nodes, subgraph_edges, subgraph_attributes, self.isdirected)\n",
    "        return subgraph\n",
    "\n",
    "    def remove_edges(self, edges_todel_list):\n",
    "        # old edges\n",
    "        edges = self.edges\n",
    "\n",
    "        # list containing the old edges without the ones to delete\n",
    "        new_edges = [edge for edge in edges if edge not in edges_todel_list]\n",
    "        \n",
    "        # set new edges as 'edges' attribute of the graph\n",
    "        self.edges = new_edges\n",
    "\n",
    "    def get_neighborhood(self, vertex): # Different cases if the graph is directed or not\n",
    "        if self.isdirected:\n",
    "            neigborhood = [edge[1] for edge in self.edges if edge[0] == vertex]\n",
    "            return neigborhood\n",
    "        \n",
    "        # not directed case\n",
    "        neigborhood = [edge[1] for edge in self.edges if edge[0] == vertex] + [edge[0] for edge in self.edges if edge[1] == vertex]\n",
    "        neigborhood = list(set(neigborhood))\n",
    "\n",
    "        return neigborhood\n",
    "    \n",
    "    def neighborhood_withedges_onlyundirected(self, vertex):\n",
    "        if not self.isdirected:\n",
    "            neigborhood_withedges = {edge[1]: edge for edge in self.edges if edge[0] == vertex}\n",
    "            neigborhood_withedges.update({edge[0]: edge for edge in self.edges if edge[1] == vertex})\n",
    "\n",
    "            return neigborhood_withedges\n",
    "\n",
    "    def indegree(self, node):\n",
    "        if self.isdirected:\n",
    "            indegree = len([edge for edge in self.edges if edge[1] == node])\n",
    "            return indegree\n",
    "        \n",
    "        \n",
    "    def to_directed(self):\n",
    "        if self.isdirected:\n",
    "            return self\n",
    "        \n",
    "        new_edges = set(edge[::-1] for edge in self.edges)\n",
    "        total_edges = list(set(self.edges).union(new_edges))\n",
    "        self.edges = total_edges\n",
    "\n",
    "        new_edges = list(new_edges)\n",
    "        updated_attributes = dict()\n",
    "        for attribute in self.edges_attributes:\n",
    "            attr_dict = self.edges_attributes[attribute]\n",
    "            new_edges_attr = dict({new_edge: attr_dict[new_edge[::-1]] for new_edge in new_edges})\n",
    "            new_edges_attr.update(attr_dict)\n",
    "            updated_attributes.update( { attribute: new_edges_attr } )\n",
    "\n",
    "        self.edges_attributes = updated_attributes\n",
    "        self.isdirected = True\n",
    "        return self\n",
    "    \n",
    "    def copy(self):\n",
    "        return graph(self.nodes, self.edges, self.edges_attributes, self.isdirected)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to extract top N papers and top N authors.\n",
    "\n",
    "- TopN papers: N papers with more citations\n",
    "- TopN authors: N authors with more publications\n",
    "\n",
    "How to retriev this info: \n",
    "- in the collaboration graph (authors' graph), the total number of publications of each author an be retrieved as the sum of the weigths adjacent to the author's node; then we can store these info and retrieve the topN authors with a heap structure, then induce a subgraph and work on it.\n",
    "- in the citation graph (papers' graph), the number of citation of each paper can be retrieved as the indegree of the node correspinding to it; then heap + topN + subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topN(G, N, flag):\n",
    "\n",
    "    # Function to retrieve the top N papers wrt number of citations\n",
    "    def topN_papers(G, N):\n",
    "        # create list of tuples (node, indegree(node)) for heapq\n",
    "        nodes_indegree = []\n",
    "        for node in G.nodes: \n",
    "            nodes_indegree.append((node, G.indegree(node)))\n",
    "        \n",
    "        # retrieve topN papers\n",
    "        nlargest = heapq.nlargest(N, nodes_indegree, key= lambda t: t[1])\n",
    "\n",
    "        # return only the nodes with the most  number of citations\n",
    "        return [t[0] for t in nlargest]\n",
    "\n",
    "    # Function to retrieve top N authors wrt their total number of publications\n",
    "    def topN_authors(G, N):\n",
    "\n",
    "        # initialize empty list \n",
    "        n_publications = []\n",
    "\n",
    "        # Retrieve all the edges' weigths of the graph\n",
    "        all_weigths = G.edges_attributes['weigths']\n",
    "\n",
    "        # retrieve number of publications for each node\n",
    "        #  dictionary {node: {Neighbor: edge between neighbor and node} for all the neighbors of the node}\n",
    "        all_nodes_neigh_withedges = dict({node: G.neighborhood_withedges_onlyundirected(node) for node in G.nodes})\n",
    "        \n",
    "        for node in G.nodes: \n",
    "            # Retrieve info on the current node\n",
    "            current_node_info = all_nodes_neigh_withedges[node]\n",
    "\n",
    "            # Construct dictionary {Neighbor: weigth of the edge between neighbor and node} for all the neighbors of the node:\n",
    "            # Initialize dictionary with all the weigths set to 0\n",
    "            neigh_weigths_dict = dict.fromkeys(current_node_info.keys(), 0)\n",
    "            # Update the dictionary with the correct weights\n",
    "            neigh_weigths_dict.update({neigh: all_weigths[current_node_info[neigh]] for neigh in current_node_info if current_node_info[neigh] in all_weigths})\n",
    "\n",
    "            # Update the list of the number of publications for each node\n",
    "            n_publications.append( ( node, sum( neigh_weigths_dict.values() ) ) )\n",
    "\n",
    "        # Retrieve top N authors wrt their total number of publications\n",
    "        nlargest = heapq.nlargest(N, n_publications, key= lambda t: t[1])\n",
    "\n",
    "        # Return only the nodes (without number of publications)\n",
    "        return [t[0] for t in nlargest]\n",
    "\n",
    "\n",
    "    if flag == 'authors':\n",
    "        topN = topN_authors(G, N)\n",
    "    elif flag == 'papers': \n",
    "        topN = topN_papers(G, N)\n",
    "    else:\n",
    "        return 'invalid flag'\n",
    "    \n",
    "    return topN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Shortest path\n",
    "\n",
    "- On the collaboration graph\n",
    "- start from source, end in sink and pass thoroguh all the nodes in the sequence in order \n",
    "\n",
    "IDEA: the shortest path that connects all the nodes in the sequence in order is the one that minimizes the number of steps for each intermediate stop to be done. Indeed, we want to minimize $\\sum_P \\sum_{e \\in E}{\\mathbb{1}_{ \\left( e \\in P \\right)}(e)} = \\sum_{i=1}^{n-1}  \\sum_{e \\in E}{\\mathbb{1}_{\\left( e \\in P_i \\right)}(e)}$ where P is a path from source to sink that goes throguh the sequence, $P_i$ is a path that goes from node $a_i$ to $a_{i+1}$ for each $i$, which ordered sequence returns exactly the path $P$ (note that such decomposition is always doable). \n",
    "\n",
    "il problema di minimo è separabile (si dice effettivamente così?) proprio per la necessità di mantenere l'ordine dei nodi nella sequenza.\n",
    "\n",
    "Shortest path intermedio implementato con Dijkistra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(G, source, sink):    \n",
    "# Initialize dictionary containing shortest paths to each node\n",
    "    paths = dict.fromkeys(G.nodes, [])\n",
    "    paths.update({source: [[]]})\n",
    "\n",
    "    #initialize BFS' exploration dictionary using distances from the source\n",
    "    distances = dict.fromkeys(G.nodes, float('inf'))\n",
    "\n",
    "    # set distance to 0 for the first node and add it to a queue\n",
    "    distances.update({source: 0})\n",
    "    q = [source]\n",
    "\n",
    "    while q != []:\n",
    "        # Extract node to explore from the queue\n",
    "        parent = q.pop(0)\n",
    "\n",
    "        # Retrieve neighborhood and \"connecting edges\"\n",
    "        neighborhood_edges = G.neighborhood_withedges_onlyundirected(parent)\n",
    "        for u in neighborhood_edges.keys():\n",
    "            # If the node has never been visited, set distance from the starting node\n",
    "            if distances[u] == float('inf'):\n",
    "                distances[u] = distances[parent] + G.edges_attributes['weigths'][ neighborhood_edges[u] ]\n",
    "\n",
    "                q.append(u)\n",
    "                \n",
    "            # Update number of shortest paths to u \n",
    "            if distances[u] == distances[parent] + G.edges_attributes['weigths'][ neighborhood_edges[u] ]:\n",
    "                paths.update({u: paths[u] + [path + [(parent, u)]] for path in paths[parent]})\n",
    "    \n",
    "    if (distances[sink] < float('inf')):\n",
    "        return (distances[sink] < float('inf')), distances[sink], paths[sink][0]\n",
    "    else:\n",
    "        return (distances[sink] < float('inf')), distances[sink], []\n",
    "\n",
    "def shortestpath_sequence(sequence: list, first_node, last_node, N, G_data=collaboration_data):\n",
    "    # extract graph class instance from the data in input \n",
    "    G = graph(G_data['nodes'], G_data['edges'], {'weigths': G_data['weigths']}, G_data['dir'])\n",
    "\n",
    "    # extract top N authors \n",
    "    topN_list = extract_topN(G, N, 'authors')\n",
    "\n",
    "    # subgraph of the top N authors\n",
    "    G_N = G.extract_subgraph(topN_list)\n",
    "\n",
    "\n",
    "    # Initialize path and path cost\n",
    "    total_path = []\n",
    "    total_cost = 0\n",
    "\n",
    "    sequence = [first_node] + sequence + [last_node]\n",
    "    \n",
    "    # check the existence of path P_i and compute shortest path and shortest path's cost if it exists\n",
    "    for i in range(len(sequence) - 1):\n",
    "        connected, path_cost, path = shortest_path(G_N, sequence[i], sequence[i+1])\n",
    "        if not connected: \n",
    "            return \"There is no such path.\"\n",
    "        \n",
    "        # Update path and path's cost if the graph is connected\n",
    "        total_path += path\n",
    "        total_cost += path_cost\n",
    "    \n",
    "    return total_cost, total_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 autori\n",
    "top1 = 332422508\n",
    "top2 = 173839695\n",
    "top3 = 2111642879\n",
    "top4 = 2146468246\n",
    "top5 = 2133227394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22,\n",
       " [(2133227394, 173839695), (173839695, 2111642879), (2111642879, 332422508)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [top2, top3]\n",
    "\n",
    "shortestpath_sequence(seq, top5, top1, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
